{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
        "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
        "execution": {
          "iopub.execute_input": "2024-12-17T15:13:26.274758Z",
          "iopub.status.busy": "2024-12-17T15:13:26.274391Z",
          "iopub.status.idle": "2024-12-17T15:13:37.205045Z",
          "shell.execute_reply": "2024-12-17T15:13:37.203996Z",
          "shell.execute_reply.started": "2024-12-17T15:13:26.274703Z"
        },
        "id": "xIBdapsQCUD0",
        "outputId": "91d73409-2411-425b-a56f-008baeabeb0e",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/Farama-Foundation/MAgent2\n",
            "  Cloning https://github.com/Farama-Foundation/MAgent2 to /tmp/pip-req-build-pfpvi3ix\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/Farama-Foundation/MAgent2 /tmp/pip-req-build-pfpvi3ix\n",
            "  Resolved https://github.com/Farama-Foundation/MAgent2 to commit b2ddd49445368cf85d4d4e1edcddae2e28aa1406\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy<2.0,>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from magent2==0.3.3) (1.26.4)\n",
            "Requirement already satisfied: pygame>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from magent2==0.3.3) (2.6.1)\n",
            "Requirement already satisfied: pettingzoo>=1.23.1 in /usr/local/lib/python3.10/dist-packages (from magent2==0.3.3) (1.24.3)\n",
            "Requirement already satisfied: gymnasium>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from pettingzoo>=1.23.1->magent2==0.3.3) (1.0.0)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=0.28.0->pettingzoo>=1.23.1->magent2==0.3.3) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=0.28.0->pettingzoo>=1.23.1->magent2==0.3.3) (4.12.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from gymnasium>=0.28.0->pettingzoo>=1.23.1->magent2==0.3.3) (0.0.4)\n",
            "fatal: destination path 'RL-final-project-AIT-3007' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "!pip install git+https://github.com/Farama-Foundation/MAgent2\n",
        "!git clone https://github.com/giangbang/RL-final-project-AIT-3007.git"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-17T15:13:46.453375Z",
          "iopub.status.busy": "2024-12-17T15:13:46.452999Z",
          "iopub.status.idle": "2024-12-17T15:13:56.426276Z",
          "shell.execute_reply": "2024-12-17T15:13:56.425438Z",
          "shell.execute_reply.started": "2024-12-17T15:13:46.453332Z"
        },
        "trusted": true,
        "id": "0jlpjoQRhVeq"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append('RL-final-project-AIT-3007')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-17T15:13:56.429630Z",
          "iopub.status.busy": "2024-12-17T15:13:56.428946Z",
          "iopub.status.idle": "2024-12-17T15:14:11.259688Z",
          "shell.execute_reply": "2024-12-17T15:14:11.258827Z",
          "shell.execute_reply.started": "2024-12-17T15:13:56.429595Z"
        },
        "id": "aBVVcwvDCUD9",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.optim.lr_scheduler as lr_scheduler\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "from collections import deque, Counter\n",
        "import os\n",
        "from magent2.environments import battle_v4\n",
        "import time"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Khởi tạo Replay Buffer\n",
        "Được dùng để lưu trữ các dữ liệu về state, action, reward, next_state và các ván đấu trước đã hoàn thành hay chưa?"
      ],
      "metadata": {
        "id": "bOUmi8bvxAGc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-17T15:14:11.261602Z",
          "iopub.status.busy": "2024-12-17T15:14:11.260909Z",
          "iopub.status.idle": "2024-12-17T15:14:11.269031Z",
          "shell.execute_reply": "2024-12-17T15:14:11.268221Z",
          "shell.execute_reply.started": "2024-12-17T15:14:11.261559Z"
        },
        "id": "7ERNxXWnCUEC",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "class ReplayBuffer:\n",
        "    def __init__(self, capacity):\n",
        "        self.capacity = capacity\n",
        "        self.buffer = deque(maxlen=capacity)\n",
        "\n",
        "    def add(self, state, action, reward, next_state, done):\n",
        "        experience = (state, action, reward, next_state, done)\n",
        "        self.buffer.append(experience)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.buffer)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.buffer[index]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-17T15:14:11.270342Z",
          "iopub.status.busy": "2024-12-17T15:14:11.270097Z",
          "iopub.status.idle": "2024-12-17T15:14:11.401713Z",
          "shell.execute_reply": "2024-12-17T15:14:11.400900Z",
          "shell.execute_reply.started": "2024-12-17T15:14:11.270319Z"
        },
        "id": "Jamz2tB-EFUy",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "class QNetwork(nn.Module):\n",
        "    def __init__(self, observation_shape, action_shape):\n",
        "        super().__init__()\n",
        "        self.cnn = nn.Sequential(\n",
        "            nn.Conv2d(observation_shape[-1], 8, kernel_size=3, padding=1, stride=1),\n",
        "            nn.ReLU(),\n",
        "            nn.Conv2d(8, 8, kernel_size=3, padding=1, stride=1),\n",
        "            nn.ReLU(),\n",
        "        )\n",
        "\n",
        "        dummy_input = torch.randn(observation_shape).permute(2, 0, 1)\n",
        "        dummy_output = self.cnn(dummy_input)\n",
        "        flatten_dim = dummy_output.view(-1).shape[0]\n",
        "\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(flatten_dim, 128),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.2),\n",
        "            nn.Linear(128, action_shape)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        assert len(x.shape) >= 3, \"only support magent input observation\"\n",
        "        out = self.cnn(x)\n",
        "\n",
        "        if len(x.shape) == 3:\n",
        "            batchsize = 1\n",
        "        else:\n",
        "            batchsize = x.shape[0]\n",
        "\n",
        "        out = out.reshape(batchsize, -1)\n",
        "        return self.fc(out)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-17T15:14:11.403154Z",
          "iopub.status.busy": "2024-12-17T15:14:11.402893Z",
          "iopub.status.idle": "2024-12-17T15:14:11.414394Z",
          "shell.execute_reply": "2024-12-17T15:14:11.413712Z",
          "shell.execute_reply.started": "2024-12-17T15:14:11.403129Z"
        },
        "id": "IIHHEVM1CUED",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "class Trainer:\n",
        "    def __init__(self, env, config_qnet=None, input_shape=None, action_shape=None, learning_rate=1e-3):\n",
        "        self.env = env\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "        self.q_network = QNetwork(input_shape, action_shape).to(self.device)\n",
        "        self.target_network = QNetwork(input_shape, action_shape).to(self.device)\n",
        "        self.target_network.load_state_dict(self.q_network.state_dict())\n",
        "\n",
        "        self.criterion = nn.MSELoss()\n",
        "        self.optimizer = optim.SGD(self.q_network.parameters(), lr=learning_rate)\n",
        "        self.steplr = torch.optim.lr_scheduler.StepLR(optimizer=self.optimizer, step_size=1, gamma=0.9)\n",
        "        self.replay_buffer = ReplayBuffer(capacity=60000)\n",
        "\n",
        "        self.gamma = 0.9\n",
        "        self.epsilon = 1.0\n",
        "        self.epsilon_min = 0.1\n",
        "        self.epsilon_decay = 0.97\n",
        "        self.update_target_every = 3\n",
        "\n",
        "        # Log to store training metrics\n",
        "        self.log = []\n",
        "\n",
        "    def select_action(self, observation, agent):\n",
        "        \"\"\"\n",
        "        Select action based on epsilon-greedy policy with potential softmax exploration.\n",
        "        \"\"\"\n",
        "        # If epsilon is greater than a random value, select a random action\n",
        "        if np.random.rand() <= self.epsilon:\n",
        "            return self.env.action_space(agent).sample()\n",
        "\n",
        "        observation = torch.FloatTensor(observation).unsqueeze(0).to(self.device)\n",
        "\n",
        "        with torch.no_grad():  # Inference mode, no gradient computation\n",
        "            q_values = self.q_network(observation)\n",
        "\n",
        "        # If we want exploration based on softmax (epsilon is scaled down)\n",
        "        if np.random.rand() <= self.epsilon * 0.1:\n",
        "            exp_q_values = torch.exp(q_values).cpu().numpy().flatten()\n",
        "            action_probabilities = exp_q_values / exp_q_values.sum()  # Normalize\n",
        "            return np.random.choice(len(action_probabilities), p=action_probabilities)\n",
        "\n",
        "        return torch.argmax(q_values, dim=1).item()\n",
        "\n",
        "    def training(self, episodes=100, batch_size=1024):\n",
        "        for episode in range(episodes):\n",
        "            self.env.reset()\n",
        "            total_reward = 0\n",
        "            reward_for_agent = {agent: 0 for agent in self.env.agents if agent.startswith('blue')}\n",
        "            prev_observation = {}\n",
        "            prev_action = {}\n",
        "            step = 0\n",
        "\n",
        "            for idx, agent in enumerate(self.env.agent_iter()):\n",
        "                step += 1\n",
        "                observation, reward, termination, truncation, info = self.env.last()\n",
        "                observation = np.transpose(observation, (2, 0, 1))\n",
        "\n",
        "                agent_handle = agent.split('_')[0]\n",
        "\n",
        "                if agent_handle == 'blue':\n",
        "                    total_reward += reward\n",
        "                    reward_for_agent[agent] += reward\n",
        "\n",
        "                if termination or truncation:\n",
        "                    action = None\n",
        "                else:\n",
        "                    if agent_handle == 'blue':\n",
        "                        action = self.select_action(observation, agent)\n",
        "                    else:\n",
        "                        action = self.env.action_space(agent).sample()\n",
        "\n",
        "                if agent_handle == 'blue':\n",
        "                    if agent in prev_observation and agent in prev_action:\n",
        "                        self.replay_buffer.add(\n",
        "                            prev_observation[agent],\n",
        "                            prev_action[agent],\n",
        "                            reward,\n",
        "                            observation,\n",
        "                            termination\n",
        "                        )\n",
        "\n",
        "                    prev_observation[agent] = observation\n",
        "                    prev_action[agent] = action\n",
        "\n",
        "                self.env.step(action)\n",
        "\n",
        "            # If replay buffer has enough data, train the model\n",
        "            if len(self.replay_buffer) >= batch_size:\n",
        "                dataloader = DataLoader(self.replay_buffer, batch_size=batch_size, shuffle=True, drop_last=True)\n",
        "                self.update_model(dataloader)\n",
        "\n",
        "            # Update target network periodically\n",
        "            if (episode + 1) % self.update_target_every == 0:\n",
        "                self.target_network.load_state_dict(self.q_network.state_dict())\n",
        "                self.steplr.step()\n",
        "\n",
        "            max_reward = max(reward_for_agent.values())\n",
        "\n",
        "            # Log metrics\n",
        "            self.log.append({\n",
        "                \"Episode\": episode + 1,\n",
        "                \"Epsilon\": self.epsilon,\n",
        "                \"Total Reward\": total_reward,\n",
        "                \"Steps\": step,\n",
        "                \"Max Reward\": max_reward,\n",
        "                \"Learning Rate\": self.steplr.get_last_lr()[0]\n",
        "            })\n",
        "\n",
        "            print(f\"Episode {episode + 1}, Epsilon: {self.epsilon:.2f}, Total Reward: {total_reward}, Steps: {step}, Max Reward: {max_reward}, lr: {self.steplr.get_last_lr()}\")\n",
        "\n",
        "            # Decay epsilon to reduce exploration over time\n",
        "            self.epsilon = max(self.epsilon * self.epsilon_decay, self.epsilon_min)\n",
        "\n",
        "    def update_model(self, dataloader):\n",
        "        self.q_network.train()\n",
        "        for states, actions, rewards, next_states, dones in dataloader:\n",
        "            # Move tensors to the correct device\n",
        "            states = states.to(dtype=torch.float32, device=self.device)\n",
        "            actions = actions.to(dtype=torch.long, device=self.device)\n",
        "            rewards = rewards.to(dtype=torch.float32, device=self.device)\n",
        "            next_states = next_states.to(dtype=torch.float32, device=self.device)\n",
        "            dones = dones.to(dtype=torch.float32, device=self.device)\n",
        "\n",
        "            # Current Q-values\n",
        "            current_q_values = self.q_network(states).gather(1, actions.unsqueeze(1)).squeeze(1)\n",
        "\n",
        "            # Next Q-values using Double Q-Learning\n",
        "            with torch.no_grad():\n",
        "                next_actions = self.q_network(next_states).argmax(1)\n",
        "                next_q_values = self.target_network(next_states).gather(1, next_actions.unsqueeze(1)).squeeze(1)\n",
        "\n",
        "            # Calculate expected Q-values\n",
        "            expected_q_values = rewards + (self.gamma * next_q_values * (1 - dones))\n",
        "\n",
        "            # Compute loss and update model\n",
        "            loss = self.criterion(current_q_values, expected_q_values)\n",
        "            self.optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            self.optimizer.step()\n",
        "\n",
        "    def plot_training_log(self):\n",
        "        if not self.log:\n",
        "            print(\"Ko có dữ liệu\")\n",
        "            return\n",
        "\n",
        "        episodes = [entry[\"Episode\"] for entry in self.log]\n",
        "        epsilons = [entry[\"Epsilon\"] for entry in self.log]\n",
        "        total_rewards = [entry[\"Total Reward\"] for entry in self.log]\n",
        "        steps = [entry[\"Steps\"] for entry in self.log]\n",
        "        max_rewards = [entry[\"Max Reward\"] for entry in self.log]\n",
        "        learning_rates = [entry[\"Learning Rate\"] for entry in self.log]\n",
        "\n",
        "        plt.figure(figsize=(12, 8))\n",
        "\n",
        "        # Plot Total Reward\n",
        "        plt.subplot(2, 3, 1)\n",
        "        plt.plot(episodes, total_rewards, label=\"Total Reward\", color=\"blue\")\n",
        "        plt.xlabel(\"Episodes\")\n",
        "        plt.ylabel(\"Total Reward\")\n",
        "        plt.title(\"Total Reward per Episode\")\n",
        "        plt.grid()\n",
        "        plt.legend()\n",
        "\n",
        "        # Plot Epsilon\n",
        "        plt.subplot(2, 3, 2)\n",
        "        plt.plot(episodes, epsilons, label=\"Epsilon\", color=\"orange\")\n",
        "        plt.xlabel(\"Episodes\")\n",
        "        plt.ylabel(\"Epsilon\")\n",
        "        plt.title(\"Epsilon Decay\")\n",
        "        plt.grid()\n",
        "        plt.legend()\n",
        "\n",
        "        # Plot Steps\n",
        "        plt.subplot(2, 3, 3)\n",
        "        plt.plot(episodes, steps, label=\"Steps\", color=\"green\")\n",
        "        plt.xlabel(\"Episodes\")\n",
        "        plt.ylabel(\"Steps\")\n",
        "        plt.title(\"Steps per Episode\")\n",
        "        plt.grid()\n",
        "        plt.legend()\n",
        "\n",
        "        # Plot Max Reward\n",
        "        plt.subplot(2, 3, 4)\n",
        "        plt.plot(episodes, max_rewards, label=\"Max Reward\", color=\"purple\")\n",
        "        plt.xlabel(\"Episodes\")\n",
        "        plt.ylabel(\"Max Reward\")\n",
        "        plt.title(\"Max Reward per Episode\")\n",
        "        plt.grid()\n",
        "        plt.legend()\n",
        "\n",
        "        # Plot Learning Rate\n",
        "        plt.subplot(2, 3, 5)\n",
        "        plt.plot(episodes, learning_rates, label=\"Learning Rate\", color=\"red\")\n",
        "        plt.xlabel(\"Episodes\")\n",
        "        plt.ylabel(\"Learning Rate\")\n",
        "        plt.title(\"Learning Rate Decay\")\n",
        "        plt.grid()\n",
        "        plt.legend()\n",
        "\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-17T15:14:11.415799Z",
          "iopub.status.busy": "2024-12-17T15:14:11.415537Z",
          "iopub.status.idle": "2024-12-17T15:14:11.429691Z",
          "shell.execute_reply": "2024-12-17T15:14:11.428836Z",
          "shell.execute_reply.started": "2024-12-17T15:14:11.415753Z"
        },
        "id": "ar8jdAafCUEE",
        "trusted": true,
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "21f92699-a138-49b3-9e10-5d47444531dc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 1, Epsilon: 1.00, Total Reward: -3265.000119926408, Steps: 159807, Max Reward: -34.400001449510455, lr: [0.001]\n",
            "Episode 2, Epsilon: 0.97, Total Reward: -3192.465116013773, Steps: 157735, Max Reward: -17.215000618249178, lr: [0.001]\n",
            "Episode 3, Epsilon: 0.94, Total Reward: -3138.1801144080237, Steps: 156471, Max Reward: -19.480000691488385, lr: [0.0009000000000000001]\n",
            "Episode 4, Epsilon: 0.91, Total Reward: -3074.385111110285, Steps: 156667, Max Reward: -23.88500088546425, lr: [0.0009000000000000001]\n",
            "Episode 5, Epsilon: 0.89, Total Reward: -2996.475107253529, Steps: 155167, Max Reward: -9.57500034570694, lr: [0.0009000000000000001]\n",
            "Episode 6, Epsilon: 0.86, Total Reward: -2878.860102759674, Steps: 155168, Max Reward: -2.315000107511878, lr: [0.0008100000000000001]\n",
            "Episode 7, Epsilon: 0.83, Total Reward: -2822.800101345405, Steps: 153407, Max Reward: -18.600001242011786, lr: [0.0008100000000000001]\n",
            "Episode 8, Epsilon: 0.81, Total Reward: -2838.2000989876688, Steps: 159315, Max Reward: -27.400001322850585, lr: [0.0008100000000000001]\n",
            "Episode 9, Epsilon: 0.78, Total Reward: -2667.9900944391266, Steps: 154426, Max Reward: -14.190000480040908, lr: [0.000729]\n",
            "Episode 10, Epsilon: 0.76, Total Reward: -2757.2000943999738, Steps: 160014, Max Reward: -26.600001087412238, lr: [0.000729]\n",
            "Episode 11, Epsilon: 0.74, Total Reward: -2618.3000892037526, Steps: 157154, Max Reward: -22.000001080334187, lr: [0.000729]\n",
            "Episode 12, Epsilon: 0.72, Total Reward: -2571.400086772628, Steps: 158134, Max Reward: -22.500000724568963, lr: [0.0006561000000000001]\n",
            "Episode 13, Epsilon: 0.69, Total Reward: -2515.2000839821994, Steps: 159318, Max Reward: -25.000001018866897, lr: [0.0006561000000000001]\n",
            "Episode 14, Epsilon: 0.67, Total Reward: -2468.200082477182, Steps: 157751, Max Reward: -21.300000944174826, lr: [0.0006561000000000001]\n",
            "Episode 15, Epsilon: 0.65, Total Reward: -2354.4900778867304, Steps: 158673, Max Reward: -8.800000296905637, lr: [0.00059049]\n",
            "Episode 16, Epsilon: 0.63, Total Reward: -2366.100077734329, Steps: 158772, Max Reward: -21.70000089984387, lr: [0.00059049]\n",
            "Episode 17, Epsilon: 0.61, Total Reward: -2298.60007433407, Steps: 159638, Max Reward: -22.30000086966902, lr: [0.00059049]\n",
            "Episode 18, Epsilon: 0.60, Total Reward: -2226.870071281679, Steps: 159531, Max Reward: -7.245000435039401, lr: [0.000531441]\n",
            "Episode 19, Epsilon: 0.58, Total Reward: -2164.575069632381, Steps: 159675, Max Reward: -10.200000809505582, lr: [0.000531441]\n",
            "Episode 20, Epsilon: 0.56, Total Reward: -2152.700067625381, Steps: 160327, Max Reward: -22.000000851228833, lr: [0.000531441]\n",
            "Episode 21, Epsilon: 0.54, Total Reward: -2099.8000660520047, Steps: 159340, Max Reward: -18.800000708550215, lr: [0.0004782969]\n",
            "Episode 22, Epsilon: 0.53, Total Reward: -2060.600063394755, Steps: 161009, Max Reward: -19.20000073686242, lr: [0.0004782969]\n",
            "Episode 23, Epsilon: 0.51, Total Reward: -2025.9000619305298, Steps: 161258, Max Reward: -17.50000066962093, lr: [0.0004782969]\n",
            "Episode 24, Epsilon: 0.50, Total Reward: -2010.825060728006, Steps: 161953, Max Reward: -21.80000066384673, lr: [0.00043046721]\n",
            "Episode 25, Epsilon: 0.48, Total Reward: -1939.3000577511266, Steps: 161673, Max Reward: -18.200000688433647, lr: [0.00043046721]\n",
            "Episode 26, Epsilon: 0.47, Total Reward: -1878.500054766424, Steps: 162162, Max Reward: -20.500000569038093, lr: [0.00043046721]\n"
          ]
        }
      ],
      "source": [
        "env = battle_v4.env(map_size=45, render_mode=None)\n",
        "\n",
        "trainer = Trainer(env, input_shape=env.observation_space(\"red_0\").shape, action_shape=env.action_space(\"red_0\").n)\n",
        "trainer.training()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trainer.plot_training_log()"
      ],
      "metadata": {
        "id": "K8osDPQStg8-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-17T15:14:11.431386Z",
          "iopub.status.busy": "2024-12-17T15:14:11.431060Z",
          "iopub.status.idle": "2024-12-17T16:00:43.993243Z",
          "shell.execute_reply": "2024-12-17T16:00:43.992349Z",
          "shell.execute_reply.started": "2024-12-17T15:14:11.431350Z"
        },
        "id": "naw_FD7ZCUEG",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "os.makedirs(\"RL-final-project-AIT-3007\", exist_ok=True)\n",
        "torch.save(trainer.q_network.state_dict(), \"RL-final-project-AIT-3007/final_blue.pt\")\n",
        "print(\"Training complete. Model saved.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "execution": {
          "iopub.execute_input": "2024-12-17T16:00:43.995833Z",
          "iopub.status.busy": "2024-12-17T16:00:43.995535Z",
          "iopub.status.idle": "2024-12-17T16:00:44.716203Z",
          "shell.execute_reply": "2024-12-17T16:00:44.715387Z",
          "shell.execute_reply.started": "2024-12-17T16:00:43.995790Z"
        },
        "id": "cITrxxHlCUEN",
        "trusted": true
      },
      "outputs": [],
      "source": [
        "import final_torch_model\n",
        "import torch_model\n",
        "\n",
        "# from torch_model import QNetwork\n",
        "try:\n",
        "    from tqdm import tqdm\n",
        "except ImportError:\n",
        "    tqdm = lambda x, *args, **kwargs: x  # Fallback: tqdm becomes a no-op\n",
        "\n",
        "def eval():\n",
        "    max_cycles = 300\n",
        "    env = battle_v4.env(map_size=45, max_cycles=max_cycles)\n",
        "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n",
        "    ### Load network\n",
        "    Network = QNetwork(observation_shape=env.observation_space(\"red_0\").shape, action_shape=env.action_space(\"red_0\").n)\n",
        "    Network.load_state_dict(\n",
        "        torch.load(\n",
        "            \"RL-final-project-AIT-3007/final_blue.pt\",\n",
        "            weights_only=True,\n",
        "            map_location=\"cpu\"\n",
        "        )\n",
        "    )\n",
        "    Network.to(device)\n",
        "\n",
        "    ### Load red_final model\n",
        "    red_final_network = final_torch_model.QNetwork(\n",
        "        env.observation_space(\"red_0\").shape, env.action_space(\"red_0\").n\n",
        "    )\n",
        "    red_final_network.load_state_dict(\n",
        "        torch.load(\n",
        "            \"RL-final-project-AIT-3007/red_final.pt\",\n",
        "            weights_only=True,\n",
        "            map_location=\"cpu\"\n",
        "        )\n",
        "    )\n",
        "    red_final_network = red_final_network.to(device)\n",
        "\n",
        "    ### Load red model\n",
        "    red_network = torch_model.QNetwork(\n",
        "        env.observation_space(\"red_0\").shape, env.action_space(\"red_0\").n\n",
        "    )\n",
        "    red_network.load_state_dict(\n",
        "        torch.load(\n",
        "            \"RL-final-project-AIT-3007/red.pt\",\n",
        "            weights_only=True,\n",
        "            map_location=\"cpu\"\n",
        "        )\n",
        "    )\n",
        "    red_network = red_network.to(device)\n",
        "\n",
        "    def random_policy(env, agent, obs):\n",
        "        return env.action_space(agent).sample()\n",
        "\n",
        "    def red_final_policy(env, agent, obs):\n",
        "        observation = (\n",
        "            torch.Tensor(obs).float().permute([2, 0, 1]).unsqueeze(0).to(device)\n",
        "        )\n",
        "        red_final_network.eval()\n",
        "        with torch.inference_mode():\n",
        "            q_values = red_final_network(observation)\n",
        "        return torch.argmax(q_values, dim=1).cpu().numpy()[0]\n",
        "\n",
        "    def red_policy(env, agent, obs):\n",
        "        observation = (\n",
        "            torch.Tensor(obs).float().permute([2, 0, 1]).unsqueeze(0).to(device)\n",
        "        )\n",
        "        red_network.eval()\n",
        "        with torch.inference_mode():\n",
        "            q_values = red_network(observation)\n",
        "        return torch.argmax(q_values, dim=1).cpu().numpy()[0]\n",
        "\n",
        "    def policy(env, agent, obs):\n",
        "        observation = (\n",
        "            torch.Tensor(obs).float().permute([2, 0, 1]).unsqueeze(0).to(device)\n",
        "        )\n",
        "        Network.eval()\n",
        "        with torch.inference_mode():\n",
        "            q_values = Network(observation)\n",
        "        return torch.argmax(q_values, dim=1).cpu().numpy()[0]\n",
        "\n",
        "    # def policy()\n",
        "\n",
        "    def run_eval(env, red_policy, blue_policy, n_episode: int = 100):\n",
        "        red_win, blue_win = [], []\n",
        "        red_tot_rw, blue_tot_rw = [], []\n",
        "        n_agent_each_team = len(env.env.action_spaces) // 2\n",
        "\n",
        "        for _ in tqdm(range(n_episode)):\n",
        "            env.reset()\n",
        "            n_kill = {\"red\": 0, \"blue\": 0}\n",
        "            red_reward, blue_reward = 0, 0\n",
        "\n",
        "            for agent in env.agent_iter():\n",
        "                observation, reward, termination, truncation, info = env.last()\n",
        "                agent_team = agent.split(\"_\")[0]\n",
        "\n",
        "                n_kill[agent_team] += (\n",
        "                    reward > 4.5\n",
        "                )  # This assumes default reward settups\n",
        "                if agent_team == \"red\":\n",
        "                    red_reward += reward\n",
        "                else:\n",
        "                    blue_reward += reward\n",
        "\n",
        "                if termination or truncation:\n",
        "                    action = None  # this agent has died\n",
        "                else:\n",
        "                    if agent_team == \"red\":\n",
        "                        action = red_policy(env, agent, observation)\n",
        "                    else:\n",
        "                        action = blue_policy(env, agent, observation)\n",
        "\n",
        "                env.step(action)\n",
        "\n",
        "            who_wins = \"red\" if n_kill[\"red\"] >= n_kill[\"blue\"] + 5 else \"draw\"\n",
        "            who_wins = \"blue\" if n_kill[\"red\"] + 5 <= n_kill[\"blue\"] else who_wins\n",
        "            red_win.append(who_wins == \"red\")\n",
        "            blue_win.append(who_wins == \"blue\")\n",
        "\n",
        "            red_tot_rw.append(red_reward / n_agent_each_team)\n",
        "            blue_tot_rw.append(blue_reward / n_agent_each_team)\n",
        "\n",
        "        return {\n",
        "            \"winrate_red\": np.mean(red_win),\n",
        "            \"winrate_blue\": np.mean(blue_win),\n",
        "            \"average_rewards_red\": np.mean(red_tot_rw),\n",
        "            \"average_rewards_blue\": np.mean(blue_tot_rw),\n",
        "        }\n",
        "\n",
        "    print(\"=\" * 20)\n",
        "    print(\"Eval with red final policy\")\n",
        "    print(\n",
        "        run_eval(\n",
        "            env=env, red_policy=red_final_policy, blue_policy=policy, n_episode=30\n",
        "        )\n",
        "    )\n",
        "\n",
        "    print(\"=\" * 20)\n",
        "    print(\"Eval with red policy\")\n",
        "    print(\n",
        "        run_eval(\n",
        "            env=env, red_policy=red_policy, blue_policy=policy, n_episode=30\n",
        "        )\n",
        "    )\n",
        "\n",
        "    print(\"=\" * 20)\n",
        "    print(\"Eval with random policy\")\n",
        "    print(\n",
        "        run_eval(\n",
        "            env=env, red_policy=random_policy, blue_policy=policy, n_episode=30\n",
        "        )\n",
        "    )\n",
        "\n",
        "    print(\"=\" * 20)\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    eval()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "isSourceIdPinned": true,
          "modelId": 193174,
          "modelInstanceId": 170865,
          "sourceId": 200290,
          "sourceType": "modelInstanceVersion"
        }
      ],
      "dockerImageVersionId": 30805,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.14"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}