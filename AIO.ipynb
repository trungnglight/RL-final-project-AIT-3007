{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2024-12-17T07:25:31.215422Z",
     "iopub.status.busy": "2024-12-17T07:25:31.214888Z",
     "iopub.status.idle": "2024-12-17T07:25:39.510892Z",
     "shell.execute_reply": "2024-12-17T07:25:39.510011Z",
     "shell.execute_reply.started": "2024-12-17T07:25:31.215385Z"
    },
    "id": "xIBdapsQCUD0",
    "outputId": "a820d986-2f2d-4594-c9e2-a04a9b2fabe7",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#%pip install magent2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-17T07:25:39.513404Z",
     "iopub.status.busy": "2024-12-17T07:25:39.513093Z",
     "iopub.status.idle": "2024-12-17T07:25:48.684477Z",
     "shell.execute_reply": "2024-12-17T07:25:48.683384Z",
     "shell.execute_reply.started": "2024-12-17T07:25:39.513376Z"
    },
    "id": "EUWe-2udCUD7",
    "outputId": "5452afd0-1258-40f5-b69c-ec222948aee0",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#%pip install pettingzoo==1.22.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-17T07:25:48.686330Z",
     "iopub.status.busy": "2024-12-17T07:25:48.685998Z",
     "iopub.status.idle": "2024-12-17T07:26:03.961196Z",
     "shell.execute_reply": "2024-12-17T07:26:03.960440Z",
     "shell.execute_reply.started": "2024-12-17T07:25:48.686302Z"
    },
    "id": "aBVVcwvDCUD9",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from magent2.environments import battle_v4\n",
    "import numpy as np\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import os\n",
    "import datetime\n",
    "from collections import deque\n",
    "import random\n",
    "import math\n",
    "from torchrl.modules import NoisyLinear\n",
    "\n",
    "class QNetworkWithNoisy(nn.Module):\n",
    "    def __init__(self, observation_shape, action_shape):\n",
    "        super().__init__()\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(observation_shape[-1], observation_shape[-1], 3),\n",
    "            nn.SiLU(),\n",
    "            nn.Conv2d(observation_shape[-1], observation_shape[-1], 3),\n",
    "            nn.SiLU(),\n",
    "        )\n",
    "        dummy_input = torch.randn(observation_shape).permute(2, 0, 1)\n",
    "        dummy_output = self.cnn(dummy_input)\n",
    "        flatten_dim = dummy_output.view(-1).shape[0]\n",
    "        self.network = nn.Sequential(\n",
    "            NoisyLinear(flatten_dim, 120),\n",
    "            # nn.LayerNorm(120),\n",
    "            nn.ReLU(),\n",
    "            NoisyLinear(120, 84),\n",
    "            # nn.LayerNorm(84),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "        self.last_layer = nn.Linear(84, action_shape)\n",
    "\n",
    "    def forward(self, x):\n",
    "        assert len(x.shape) >= 3, \"only support magent input observation\"\n",
    "        x = x.permute(0, 3, 1, 2)\n",
    "        x = self.cnn(x)\n",
    "        if len(x.shape) == 3:\n",
    "            batchsize = 1\n",
    "        else:\n",
    "            batchsize = x.shape[0]\n",
    "        x = x.reshape(batchsize, -1)\n",
    "        x = self.network(x)\n",
    "        self.last_latent = x\n",
    "        return self.last_layer(x)\n",
    "\n",
    "\n",
    "    def reset_noise(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, NoisyLinear):\n",
    "                m.reset_noise()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-17T07:26:03.963660Z",
     "iopub.status.busy": "2024-12-17T07:26:03.962587Z",
     "iopub.status.idle": "2024-12-17T07:26:03.970915Z",
     "shell.execute_reply": "2024-12-17T07:26:03.969854Z",
     "shell.execute_reply.started": "2024-12-17T07:26:03.963616Z"
    },
    "id": "7ERNxXWnCUEC",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class RedQNetwork(nn.Module):\n",
    "    def __init__(self, observation_shape, action_shape):\n",
    "        super().__init__()\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(observation_shape[-1], observation_shape[-1], 3),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(observation_shape[-1], observation_shape[-1], 3),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        dummy_input = torch.randn(observation_shape).permute(2, 0, 1)\n",
    "        dummy_output = self.cnn(dummy_input)\n",
    "        flatten_dim = dummy_output.view(-1).shape[0]\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(flatten_dim, 120),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(120, 84),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(84, action_shape),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        assert len(x.shape) >= 3, \"only support magent input observation\"\n",
    "        x = self.cnn(x)\n",
    "        if len(x.shape) == 3:\n",
    "            batchsize = 1\n",
    "        else:\n",
    "            batchsize = x.shape[0]\n",
    "        x = x.reshape(batchsize, -1)\n",
    "        return self.network(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-17T07:26:03.973882Z",
     "iopub.status.busy": "2024-12-17T07:26:03.973502Z",
     "iopub.status.idle": "2024-12-17T07:26:04.005035Z",
     "shell.execute_reply": "2024-12-17T07:26:04.003991Z",
     "shell.execute_reply.started": "2024-12-17T07:26:03.973853Z"
    },
    "id": "Jamz2tB-EFUy",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class FinalRedQNetwork(nn.Module):\n",
    "    def __init__(self, observation_shape, action_shape):\n",
    "        super().__init__()\n",
    "        self.cnn = nn.Sequential(\n",
    "            nn.Conv2d(observation_shape[-1], observation_shape[-1], 3),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(observation_shape[-1], observation_shape[-1], 3),\n",
    "            nn.ReLU(),\n",
    "        )\n",
    "        dummy_input = torch.randn(observation_shape).permute(2, 0, 1)\n",
    "        dummy_output = self.cnn(dummy_input)\n",
    "        flatten_dim = dummy_output.view(-1).shape[0]\n",
    "        self.network = nn.Sequential(\n",
    "            nn.Linear(flatten_dim, 120),\n",
    "            # nn.LayerNorm(120),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(120, 84),\n",
    "            # nn.LayerNorm(84),\n",
    "            nn.Tanh(),\n",
    "        )\n",
    "        self.last_layer = nn.Linear(84, action_shape)\n",
    "\n",
    "    def forward(self, x):\n",
    "        assert len(x.shape) >= 3, \"only support magent input observation\"\n",
    "        x = self.cnn(x)\n",
    "        if len(x.shape) == 3:\n",
    "            batchsize = 1\n",
    "        else:\n",
    "            batchsize = x.shape[0]\n",
    "        x = x.reshape(batchsize, -1)\n",
    "        x = self.network(x)\n",
    "        self.last_latent = x\n",
    "        return self.last_layer(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-17T07:26:04.022761Z",
     "iopub.status.busy": "2024-12-17T07:26:04.022522Z",
     "iopub.status.idle": "2024-12-17T07:26:04.037718Z",
     "shell.execute_reply": "2024-12-17T07:26:04.036924Z",
     "shell.execute_reply.started": "2024-12-17T07:26:04.022739Z"
    },
    "id": "IIHHEVM1CUED",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# --- Prioritized Replay Buffer ---\n",
    "class PrioritizedReplayBuffer:\n",
    "    def __init__(self, capacity, alpha=0.6, beta_start=0.4, beta_frames=100000):\n",
    "        self.capacity = capacity\n",
    "        self.buffer = []\n",
    "        self.priorities = np.zeros((capacity,), dtype=np.float32)\n",
    "        self.alpha = alpha\n",
    "        self.beta_start = beta_start\n",
    "        self.beta_frames = beta_frames\n",
    "        self.frame = 1\n",
    "        self.pos = 0\n",
    "        self.max_priority = 1.0\n",
    "\n",
    "    def push(self, state, action, reward, next_state, done):\n",
    "        if len(self.buffer) < self.capacity:\n",
    "            self.buffer.append((state, action, reward, next_state, done))\n",
    "        else:\n",
    "            self.buffer[self.pos] = (state, action, reward, next_state, done)\n",
    "\n",
    "        self.priorities[self.pos] = self.max_priority\n",
    "        self.pos = (self.pos + 1) % self.capacity\n",
    "\n",
    "    def sample(self, batch_size):\n",
    "        if len(self.buffer) == self.capacity:\n",
    "            prios = self.priorities\n",
    "        else:\n",
    "            prios = self.priorities[:self.pos]\n",
    "\n",
    "        probs = prios ** self.alpha\n",
    "        probs /= probs.sum()\n",
    "\n",
    "        indices = np.random.choice(len(self.buffer), batch_size, p=probs)\n",
    "        samples = [self.buffer[idx] for idx in indices]\n",
    "\n",
    "        beta = self.beta_start + (1.0 - self.beta_start) * self.frame / self.beta_frames\n",
    "        self.frame = min(self.frame + 1, self.beta_frames)\n",
    "\n",
    "        weights = (len(self.buffer) * probs[indices]) ** (-beta)\n",
    "        weights /= weights.max()\n",
    "        weights = np.array(weights, dtype=np.float32)\n",
    "\n",
    "        state, action, reward, next_state, done = zip(*samples)\n",
    "        return state, action, reward, next_state, done, indices, weights\n",
    "\n",
    "    def update_priorities(self, batch_indices, batch_priorities):\n",
    "        for idx, prio in zip(batch_indices, batch_priorities):\n",
    "            self.priorities[idx] = prio\n",
    "            self.max_priority = max(self.max_priority, prio)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.buffer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-17T07:26:04.039194Z",
     "iopub.status.busy": "2024-12-17T07:26:04.038895Z",
     "iopub.status.idle": "2024-12-17T07:26:04.054412Z",
     "shell.execute_reply": "2024-12-17T07:26:04.053671Z",
     "shell.execute_reply.started": "2024-12-17T07:26:04.039148Z"
    },
    "id": "ar8jdAafCUEE",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def evaluate_checkpoint(q_network, red_q_network, pretrain_policy, num_episodes=5):\n",
    "    env = battle_v4.parallel_env(map_size=45, max_cycles=1000, minimap_mode=False)\n",
    "    total_steps = 0\n",
    "    total_rewards_random = 0\n",
    "    total_rewards_pretrained = 0\n",
    "\n",
    "    for _ in range(num_episodes):\n",
    "        # --- Evaluate against random policy ---\n",
    "        observations = env.reset()\n",
    "        done_agents = set()\n",
    "        episode_reward_random = 0\n",
    "        while True:\n",
    "            actions = {}\n",
    "            red_agents = [agent for agent in env.agents if agent.startswith(\"red_\") and agent not in done_agents]\n",
    "            blue_agents = [agent for agent in env.agents if agent.startswith(\"blue_\") and agent not in done_agents]\n",
    "\n",
    "            if red_agents:\n",
    "                for agent in red_agents:\n",
    "                    actions[agent] = env.action_spaces[agent].sample()  # Random action\n",
    "\n",
    "            if blue_agents:\n",
    "                states_blue = torch.stack([\n",
    "                    torch.tensor(observations[agent], dtype=torch.float32)\n",
    "                    for agent in blue_agents\n",
    "                ]).to(device)\n",
    "                with torch.no_grad():\n",
    "                    q_network_blue.reset_noise()\n",
    "                    q_values_blue = q_network(states_blue)\n",
    "                    selected_actions_blue = torch.argmax(q_values_blue, dim=1)\n",
    "\n",
    "                for idx, agent in enumerate(blue_agents):\n",
    "                    actions[agent] = selected_actions_blue[idx].item()\n",
    "\n",
    "            next_obs, rewards, terminations, truncations, infos = env.step(actions)\n",
    "            done_agents.update([agent for agent, terminated in terminations.items() if terminated])\n",
    "            observations = next_obs\n",
    "            episode_reward_random += sum(rewards.get(agent, 0.0) for agent in blue_agents)\n",
    "            if all(terminations.values()) or all(truncations.values()):\n",
    "                total_rewards_random += episode_reward_random\n",
    "                break\n",
    "\n",
    "        # --- Evaluate against pretrained policy ---\n",
    "        observations = env.reset()\n",
    "        done_agents = set()\n",
    "        episode_reward_pretrained = 0\n",
    "        while True:\n",
    "            actions = {}\n",
    "            red_agents = [agent for agent in env.agents if agent.startswith(\"red_\") and agent not in done_agents]\n",
    "            blue_agents = [agent for agent in env.agents if agent.startswith(\"blue_\") and agent not in done_agents]\n",
    "\n",
    "            if red_agents:\n",
    "                for agent in red_agents:\n",
    "                    actions[agent] = pretrain_policy(env, agent, observations[agent])  # Pretrained action\n",
    "\n",
    "            if blue_agents:\n",
    "                states_blue = torch.stack([\n",
    "                    torch.tensor(observations[agent], dtype=torch.float32)\n",
    "                    for agent in blue_agents\n",
    "                ]).to(device)\n",
    "                with torch.no_grad():\n",
    "                    q_network_blue.reset_noise()\n",
    "                    q_values_blue = q_network(states_blue)\n",
    "                    selected_actions_blue = torch.argmax(q_values_blue, dim=1)\n",
    "\n",
    "                for idx, agent in enumerate(blue_agents):\n",
    "                    actions[agent] = selected_actions_blue[idx].item()\n",
    "\n",
    "            next_obs, rewards, terminations, truncations, infos = env.step(actions)\n",
    "            done_agents.update([agent for agent, terminated in terminations.items() if terminated])\n",
    "            observations = next_obs\n",
    "            episode_reward_pretrained += sum(rewards.get(agent, 0.0) for agent in blue_agents)\n",
    "            if all(terminations.values()) or all(truncations.values()):\n",
    "                total_rewards_pretrained += episode_reward_pretrained\n",
    "                break\n",
    "\n",
    "    avg_reward_random = total_rewards_random / num_episodes\n",
    "    avg_reward_pretrained = total_rewards_pretrained / num_episodes\n",
    "    avg_reward = (avg_reward_random + avg_reward_pretrained) / 2\n",
    "\n",
    "    return avg_reward, avg_reward_random, avg_reward_pretrained\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-12-17T07:26:04.056075Z",
     "iopub.status.busy": "2024-12-17T07:26:04.055717Z"
    },
    "id": "naw_FD7ZCUEG",
    "outputId": "dab4113d-089e-4189-8943-8b7b5a9df8b9",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# --- Initialize environment and device ---\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# --- TensorBoard writer ---\n",
    "log_dir = os.path.join(\"runs\", \"Dueling_Noisy_DDQN_BattleV4_PER_\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\"))\n",
    "writer = SummaryWriter(log_dir=log_dir)\n",
    "\n",
    "# --- Initialize the environment ---\n",
    "env = battle_v4.parallel_env(map_size=45, max_cycles=1000, minimap_mode=False)\n",
    "\n",
    "sample_observation = env.observation_spaces[env.agents[0]].shape\n",
    "state_space = sample_observation\n",
    "action_space = env.action_spaces[env.agents[0]].n\n",
    "\n",
    "# --- Initialize networks and optimizers ---\n",
    "q_network_blue = QNetworkWithNoisy(state_space, action_space).to(device)\n",
    "target_network_blue = QNetworkWithNoisy(state_space, action_space).to(device)\n",
    "target_network_blue.load_state_dict(q_network_blue.state_dict())\n",
    "target_network_blue.eval()\n",
    "\n",
    "optimizer_blue = optim.Adam(q_network_blue.parameters(), lr=0.0005)\n",
    "\n",
    "# --- Hyperparameters ---\n",
    "gamma = 0.99\n",
    "num_episodes = 250\n",
    "max_steps_per_episode = 1000\n",
    "checkpoint_interval = 5\n",
    "polyak_tau = 0.005\n",
    "replay_buffer_size = 500000\n",
    "batch_size = 4096\n",
    "num_val_episodes = 5\n",
    "\n",
    "# --- Initialize prioritized replay buffers ---\n",
    "replay_buffer_blue = PrioritizedReplayBuffer(replay_buffer_size)\n",
    "\n",
    "red_q_network = RedQNetwork(state_space, action_space).to(device)  # Assuming same architecture\n",
    "red_q_network.load_state_dict(torch.load(\"red.pt\", map_location=device, weights_only = True))\n",
    "red_q_network.eval()\n",
    "\n",
    "def pretrain_policy(env, agent, obs):\n",
    "    observation = (torch.Tensor(obs).float().permute([2, 0, 1]).unsqueeze(0).to(device))\n",
    "    with torch.no_grad():\n",
    "        red_q_values = red_q_network(observation)\n",
    "    return torch.argmax(red_q_values, dim=1).cpu().numpy()[0]\n",
    "# --- Training loop ---\n",
    "best_val = -float('inf')\n",
    "best_checkpoint = \"\"\n",
    "for episode in range(0, num_episodes + 1):\n",
    "    observations = env.reset()\n",
    "    total_reward_blue = 0\n",
    "    done_agents = set()\n",
    "\n",
    "    for step in range(max_steps_per_episode):\n",
    "        actions = {}\n",
    "        red_agents = [agent for agent in env.agents if agent.startswith(\"red_\") and agent not in done_agents]\n",
    "        blue_agents = [agent for agent in env.agents if agent.startswith(\"blue_\") and agent not in done_agents]\n",
    "\n",
    "        # --- Process all blue agents ---\n",
    "        if blue_agents:\n",
    "            states_blue = torch.stack([\n",
    "                torch.tensor(observations[agent], dtype=torch.float32)\n",
    "                for agent in blue_agents\n",
    "            ]).to(device)\n",
    "\n",
    "            # --- Select actions with Noisy Networks ---\n",
    "            q_network_blue.reset_noise()\n",
    "            with torch.no_grad():\n",
    "                q_values_blue = q_network_blue(states_blue)\n",
    "                selected_actions_blue = torch.argmax(q_values_blue, dim=1)\n",
    "\n",
    "            for idx, agent in enumerate(blue_agents):\n",
    "                actions[agent] = selected_actions_blue[idx].item()\n",
    "\n",
    "        # --- Process all red agents ---\n",
    "        if red_agents:\n",
    "            selected_actions_red = torch.randint(0, action_space, (len(red_agents),), device=device)\n",
    "            for idx, agent in enumerate(red_agents):\n",
    "                actions[agent] = selected_actions_red[idx].item()\n",
    "\n",
    "        # --- Take step in environment ---\n",
    "\n",
    "        next_obs, rewards, terminations, truncations, infos = env.step(actions)\n",
    "        # --- Update total rewards ---\n",
    "        total_reward_blue += sum(rewards.get(agent, 0.0) for agent in blue_agents)\n",
    "        # --- Store experiences in replay buffers ---\n",
    "        for agent in blue_agents:\n",
    "            next_state = next_obs.get(agent) if not terminations.get(agent, False) else observations[agent]  # Use current state as next state if agent is done\n",
    "            replay_buffer_blue.push(\n",
    "                observations[agent],\n",
    "                actions[agent],\n",
    "                rewards.get(agent, 0.0),\n",
    "                next_state,\n",
    "                terminations.get(agent, False)  # Only consider termination as done\n",
    "            )\n",
    "        # --- Train blue network on a batch sampled from replay buffer ---\n",
    "        if len(replay_buffer_blue) >= batch_size:\n",
    "            state_batch, action_batch, reward_batch, next_state_batch, done_batch, indices, weights = replay_buffer_blue.sample(batch_size)\n",
    "\n",
    "            current_states_blue = torch.tensor(np.array(state_batch), dtype=torch.float32).to(device)\n",
    "            current_actions_blue = torch.tensor(action_batch, dtype=torch.long).unsqueeze(1).to(device)\n",
    "            next_states_blue = torch.tensor(np.array(next_state_batch), dtype=torch.float32).to(device)\n",
    "            rewards_blue = torch.tensor(reward_batch, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "            dones_blue = torch.tensor(done_batch, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "            weights_tensor_blue = torch.tensor(weights, dtype=torch.float32).unsqueeze(1).to(device)\n",
    "\n",
    "            q_network_blue.reset_noise()\n",
    "            target_network_blue.reset_noise()\n",
    "            current_q_values_blue = q_network_blue(current_states_blue).gather(1, current_actions_blue)\n",
    "            with torch.no_grad():\n",
    "                actions = q_network_blue(next_states_blue).max(1, keepdim=True)[1]\n",
    "                next_q_values_blue = target_network_blue(next_states_blue).gather(1, actions)\n",
    "                target_q_values_blue = rewards_blue + gamma * next_q_values_blue * (1 - dones_blue)\n",
    "\n",
    "            td_errors = (target_q_values_blue - current_q_values_blue).abs().detach().cpu().numpy()\n",
    "            new_priorities = td_errors + 1e-6\n",
    "            replay_buffer_blue.update_priorities(indices, new_priorities)\n",
    "\n",
    "            loss_blue = (weights_tensor_blue * nn.SmoothL1Loss(reduction='none')(current_q_values_blue, target_q_values_blue)).mean()\n",
    "\n",
    "            optimizer_blue.zero_grad()\n",
    "            loss_blue.backward()\n",
    "            nn.utils.clip_grad_norm_(q_network_blue.parameters(), 1.0)\n",
    "            optimizer_blue.step()\n",
    "\n",
    "            writer.add_scalar('Loss/Blue', loss_blue.item(), episode * max_steps_per_episode + step)\n",
    "\n",
    "        # --- Soft update target networks ---\n",
    "        for target_param, param in zip(target_network_blue.parameters(), q_network_blue.parameters()):\n",
    "            target_param.data.copy_(polyak_tau * param.data + (1 - polyak_tau) * target_param.data)\n",
    "\n",
    "        observations = next_obs\n",
    "        done_agents.update([agent for agent, terminated in terminations.items() if terminated])\n",
    "\n",
    "        if all(terminations.values()) or (truncations and all(truncations.values())):\n",
    "            break\n",
    "\n",
    "    # --- Log episode statistics ---\n",
    "    # --- Log episode statistics ---\n",
    "    writer.add_scalar('Total Reward/Blue', total_reward_blue, episode)\n",
    "    print(f\"Episode {episode}/{num_episodes}, Total Reward Blue: {total_reward_blue:.2f}\")\n",
    "    # --- Save checkpoints and evaluate ---\n",
    "    if episode % checkpoint_interval == 0:\n",
    "        checkpoint_path = f\"blue_agent_dueling_ddqn_noise_per_ep{episode}.pth\"\n",
    "        torch.save(q_network_blue.state_dict(), checkpoint_path)\n",
    "        val_reward, val_reward_random, val_reward_pretrained = evaluate_checkpoint(q_network_blue, red_q_network, pretrain_policy, num_val_episodes)\n",
    "        writer.add_scalar('Validation/Average Reward', val_reward, episode)\n",
    "        writer.add_scalar('Validation/Average Reward vs Random', val_reward_random, episode)\n",
    "        writer.add_scalar('Validation/Average Reward vs Pretrained', val_reward_pretrained, episode)\n",
    "        print(f\"Episode {episode}: Validation average reward = {val_reward:.2f}, \"\n",
    "              f\"vs Random = {val_reward_random:.2f}, vs Pretrained = {val_reward_pretrained:.2f}\")\n",
    "        if val_reward >= best_val:\n",
    "            best_val = val_reward\n",
    "            if best_checkpoint:\n",
    "                os.remove(best_checkpoint)\n",
    "            best_checkpoint = checkpoint_path\n",
    "            print(f\"New best checkpoint saved at {best_checkpoint}\")\n",
    "        if val_reward < best_val:\n",
    "            os.remove(checkpoint_path)\n",
    "if best_checkpoint:\n",
    "    best_checkpoint_final = \"blue_agent_dueling_ddqn_noise_per_best.pth\"\n",
    "    os.rename(best_checkpoint, best_checkpoint_final)\n",
    "    print(f\"Training complete. Best model saved at {best_checkpoint_final}\")\n",
    "else:\n",
    "    torch.save(q_network_blue.state_dict(), \"blue_agent_dueling_ddqn_noise_per_final.pth\")\n",
    "    print(f\"Training complete. Final model saved at blue_agent_dueling_ddqn_per_final.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(q_network_blue.state_dict(), \"blue_agent_dueling_ddqn_noise_per_final.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cITrxxHlCUEN",
    "outputId": "4931a59f-6cc7-479b-c04d-40b4925da9e0",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from tensorboard.backend.event_processing.event_accumulator import EventAccumulator\n",
    "import os\n",
    "\n",
    "# --- Function to extract data from TensorBoard logs ---\n",
    "def extract_data_from_logs(log_dir, tags):\n",
    "    event_acc = EventAccumulator(log_dir)\n",
    "    event_acc.Reload()\n",
    "    data = {tag: [] for tag in tags}\n",
    "\n",
    "    for tag in tags:\n",
    "        events = event_acc.Scalars(tag)\n",
    "        for event in events:\n",
    "            data[tag].append((event.step, event.value))\n",
    "\n",
    "    return data\n",
    "\n",
    "# --- Set log directory ---\n",
    "log_dir = \"runs\"  # Replace with the directory where TensorBoard logs are stored\n",
    "latest_run = max([os.path.join(log_dir, d) for d in os.listdir(log_dir)], key=os.path.getmtime)\n",
    "\n",
    "# --- Tags to plot ---\n",
    "tags_to_plot = ['Total Reward/Blue','Validation/Average Reward','Validation/Average Reward vs Random','Validation/Average Reward vs Pretrained']\n",
    "data = extract_data_from_logs(latest_run, tags_to_plot)\n",
    "\n",
    "# --- Plot data ---\n",
    "plt.figure(figsize=(15, 10))\n",
    "for tag in tags_to_plot:\n",
    "    steps, values = zip(*data[tag])\n",
    "    plt.plot(steps, values, label=tag)\n",
    "\n",
    "plt.xlabel(\"Steps\")\n",
    "plt.ylabel(\"Value\")\n",
    "plt.title(\"Training Metrics\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Y6234Fc4NETi",
    "outputId": "1bb02e6c-1fd3-432d-93eb-e79461aca6e8",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Initialize environment and device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "import cv2\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize environment\n",
    "    env = battle_v4.env(map_size=45, render_mode=\"rgb_array\")\n",
    "    vid_dir = \"video\"\n",
    "    os.makedirs(vid_dir, exist_ok=True)\n",
    "    fps = 35\n",
    "\n",
    "    env.reset()\n",
    "    frames = []\n",
    "\n",
    "    # Load the pretrained model\n",
    "    model_path = \"blue_agent_dueling_ddqn_noise_per_best.pth\"  # Adjust path as needed\n",
    "    sample_observation = env.observation_spaces[env.agents[0]].shape\n",
    "    state_space = sample_observation  # Dynamic observation shape\n",
    "    action_space = env.action_spaces[env.agents[0]].n\n",
    "\n",
    "    blue_q_network = QNetworkWithNoisy(state_space, action_space).to(device)\n",
    "    checkpoint = torch.load(model_path, map_location=device, weights_only = True)\n",
    "    blue_q_network.load_state_dict(checkpoint)\n",
    "    blue_q_network.eval()\n",
    "\n",
    "    red_q_network = RedQNetwork(state_space, action_space).to(device)\n",
    "    red_q_network.load_state_dict(torch.load(\"red.pt\", weights_only=True, map_location=\"cpu\"))\n",
    "    red_q_network.eval()\n",
    "\n",
    "    for agent in env.agent_iter():\n",
    "        observation, reward, termination, truncation, info = env.last()\n",
    "\n",
    "        if termination or truncation:\n",
    "            action = None  # This agent has died\n",
    "        else:\n",
    "            if agent.startswith(\"red\"):\n",
    "                observation = (\n",
    "                    torch.Tensor(observation).float().permute([2, 0, 1]).unsqueeze(0).to(device)\n",
    "                )\n",
    "                with torch.no_grad():\n",
    "                   red_q_values = red_q_network(observation)\n",
    "                   action = torch.argmax(red_q_values, dim=1).cpu().numpy()[0]\n",
    "            else:\n",
    "                obs = torch.tensor(observation, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "                with torch.no_grad():\n",
    "                    blue_q_values = blue_q_network(obs)\n",
    "                action = int(torch.argmax(blue_q_values, dim=1).item())\n",
    "        env.step(action)\n",
    "\n",
    "        if agent == \"blue_12\" or agent == \"blue_77\" or agent == \"blue_37\" or agent == \"blue_7\":\n",
    "            frames.append(env.render())\n",
    "\n",
    "    # Save the video\n",
    "    height, width, _ = frames[0].shape\n",
    "    out = cv2.VideoWriter(\n",
    "        os.path.join(vid_dir, f\"pretrained_agents.mp4\"),\n",
    "        cv2.VideoWriter_fourcc(*\"mp4v\"),\n",
    "        fps,\n",
    "        (width, height),\n",
    "    )\n",
    "    for frame in frames:\n",
    "        frame_bgr = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
    "        out.write(frame_bgr)\n",
    "    out.release()\n",
    "    print(\"Done recording pretrained agents\")\n",
    "\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "G38W3hdYWE-0",
    "outputId": "e4627b67-3e1a-48d3-a345-69ea59d98467",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "# Initialize environment and device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "import cv2\n",
    "if __name__ == \"__main__\":\n",
    "    # Initialize environment\n",
    "    env = battle_v4.env(map_size=45, render_mode=\"rgb_array\")\n",
    "    vid_dir = \"video\"\n",
    "    os.makedirs(vid_dir, exist_ok=True)\n",
    "    fps = 35\n",
    "\n",
    "    env.reset()\n",
    "    frames = []\n",
    "\n",
    "    # Load the pretrained model\n",
    "    model_path = \"blue_agent_dueling_ddqn_noise_per_best.pth\"  # Adjust path as needed\n",
    "    sample_observation = env.observation_spaces[env.agents[0]].shape\n",
    "    state_space = sample_observation  # Dynamic observation shape\n",
    "    action_space = env.action_spaces[env.agents[0]].n\n",
    "\n",
    "    blue_q_network = QNetworkWithNoisy(state_space, action_space).to(device)\n",
    "    checkpoint = torch.load(model_path, map_location=device, weights_only = True)\n",
    "    blue_q_network.load_state_dict(checkpoint)\n",
    "    blue_q_network.eval()\n",
    "\n",
    "    red_final_q_network = FinalRedQNetwork(state_space, action_space).to(device)\n",
    "    red_final_q_network.load_state_dict(torch.load(\"red_final.pt\", weights_only=True, map_location=\"cpu\"))\n",
    "    red_final_q_network.eval()\n",
    "\n",
    "    for agent in env.agent_iter():\n",
    "        observation, reward, termination, truncation, info = env.last()\n",
    "\n",
    "        if termination or truncation:\n",
    "            action = None  # This agent has died\n",
    "        else:\n",
    "            if agent.startswith(\"red\"):\n",
    "                observation = (\n",
    "                    torch.Tensor(observation).float().permute([2, 0, 1]).unsqueeze(0).to(device)\n",
    "                )\n",
    "                with torch.no_grad():\n",
    "                   red_final_q_values = red_final_q_network(observation)\n",
    "                   action = torch.argmax(red_final_q_values, dim=1).cpu().numpy()[0]\n",
    "            else:\n",
    "                obs = torch.tensor(observation, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "                with torch.no_grad():\n",
    "                    blue_q_values = blue_q_network(obs)\n",
    "                action = int(torch.argmax(blue_q_values, dim=1).item())\n",
    "        env.step(action)\n",
    "\n",
    "        if agent == \"blue_12\" or agent == \"blue_77\" or agent == \"blue_37\" or agent == \"blue_7\":\n",
    "            frames.append(env.render())\n",
    "\n",
    "    # Save the video\n",
    "    height, width, _ = frames[0].shape\n",
    "    out = cv2.VideoWriter(\n",
    "        os.path.join(vid_dir, f\"pretrained_final_agents.mp4\"),\n",
    "        cv2.VideoWriter_fourcc(*\"mp4v\"),\n",
    "        fps,\n",
    "        (width, height),\n",
    "    )\n",
    "    for frame in frames:\n",
    "        frame_bgr = cv2.cvtColor(frame, cv2.COLOR_RGB2BGR)\n",
    "        out.write(frame_bgr)\n",
    "    out.release()\n",
    "    print(\"Done recording pretrained final agents\")\n",
    "\n",
    "    env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uyh-3Ro8NE_3",
    "outputId": "31c7b13a-9796-4033-e268-2493ef3c8c4a",
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "from magent2.environments import battle_v4\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "try:\n",
    "    from tqdm import tqdm\n",
    "except ImportError:\n",
    "    tqdm = lambda x, *args, **kwargs: x  # Fallback: tqdm becomes a no-op\n",
    "\n",
    "\n",
    "def eval():\n",
    "    max_cycles = 300\n",
    "    env = battle_v4.env(map_size=45, max_cycles=max_cycles)\n",
    "    device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    def random_policy(env, agent, obs):\n",
    "        return env.action_space(agent).sample()\n",
    "\n",
    "    red_q_network = RedQNetwork(state_space, action_space).to(device)\n",
    "    red_q_network.load_state_dict(torch.load(\"red.pt\", weights_only=True, map_location=\"cpu\"))\n",
    "\n",
    "    red_final_q_network = FinalRedQNetwork(state_space, action_space).to(device)\n",
    "    red_final_q_network.load_state_dict(torch.load(\"red_final.pt\", weights_only=True, map_location=\"cpu\"))\n",
    "\n",
    "    blue_q_network = QNetworkWithNoisy(state_space, action_space).to(device)\n",
    "    blue_q_network.load_state_dict(torch.load(\"blue_agent_dueling_ddqn_noise_per_best.pth\", map_location=device, weights_only=True))\n",
    "\n",
    "\n",
    "    def pretrain_policy(env, agent, obs):\n",
    "        observation = (\n",
    "            torch.Tensor(obs).float().permute([2, 0, 1]).unsqueeze(0).to(device)\n",
    "        )\n",
    "        with torch.no_grad():\n",
    "            red_q_values = red_q_network(observation)\n",
    "        return torch.argmax(red_q_values, dim=1).cpu().numpy()[0]\n",
    "\n",
    "    def final_pretrain_policy(env, agent, obs):\n",
    "        observation = (\n",
    "            torch.Tensor(obs).float().permute([2, 0, 1]).unsqueeze(0).to(device)\n",
    "        )\n",
    "        with torch.no_grad():\n",
    "            red_final_q_values = red_final_q_network(observation)\n",
    "        return torch.argmax(red_final_q_values, dim=1).cpu().numpy()[0]\n",
    "\n",
    "    def my_model_pretrain_policy(env, agent, obs):\n",
    "        observation = torch.tensor(obs, dtype=torch.float32).unsqueeze(0).to(device)\n",
    "        with torch.no_grad():\n",
    "          blue_q_values = blue_q_network(observation)\n",
    "        return int(torch.argmax(blue_q_values, dim=1).item())\n",
    "    def run_eval(env, red_policy, blue_policy, n_episode: int = 100):\n",
    "        red_win, blue_win = [], []\n",
    "        red_tot_rw, blue_tot_rw = [], []\n",
    "        n_agent_each_team = len(env.env.action_spaces) // 2\n",
    "\n",
    "        for _ in tqdm(range(n_episode)):\n",
    "            env.reset()\n",
    "            n_kill = {\"red\": 0, \"blue\": 0}\n",
    "            red_reward, blue_reward = 0, 0\n",
    "\n",
    "            for agent in env.agent_iter():\n",
    "                observation, reward, termination, truncation, info = env.last()\n",
    "                agent_team = agent.split(\"_\")[0]\n",
    "\n",
    "                n_kill[agent_team] += (\n",
    "                    reward > 4.5\n",
    "                )  # This assumes default reward settups\n",
    "                if agent_team == \"red\":\n",
    "                    red_reward += reward\n",
    "                else:\n",
    "                    blue_reward += reward\n",
    "\n",
    "                if termination or truncation:\n",
    "                    action = None  # this agent has died\n",
    "                else:\n",
    "                    if agent_team == \"red\":\n",
    "                        action = red_policy(env, agent, observation)\n",
    "                    else:\n",
    "                        action = blue_policy(env, agent, observation)\n",
    "\n",
    "                env.step(action)\n",
    "\n",
    "            who_wins = \"red\" if n_kill[\"red\"] >= n_kill[\"blue\"] + 5 else \"draw\"\n",
    "            who_wins = \"blue\" if n_kill[\"red\"] + 5 <= n_kill[\"blue\"] else who_wins\n",
    "            red_win.append(who_wins == \"red\")\n",
    "            blue_win.append(who_wins == \"blue\")\n",
    "\n",
    "            red_tot_rw.append(red_reward / n_agent_each_team)\n",
    "            blue_tot_rw.append(blue_reward / n_agent_each_team)\n",
    "\n",
    "        return {\n",
    "            \"winrate_red\": np.mean(red_win),\n",
    "            \"winrate_blue\": np.mean(blue_win),\n",
    "            \"average_rewards_red\": np.mean(red_tot_rw),\n",
    "            \"average_rewards_blue\": np.mean(blue_tot_rw),\n",
    "        }\n",
    "\n",
    "    print(\"=\" * 20)\n",
    "    print(\"Eval with random policy\")\n",
    "    print(\n",
    "        run_eval(\n",
    "            env=env, red_policy=random_policy, blue_policy=my_model_pretrain_policy, n_episode=30\n",
    "        )\n",
    "    )\n",
    "    print(\"=\" * 20)\n",
    "\n",
    "    print(\"Eval with trained policy\")\n",
    "    print(\n",
    "        run_eval(\n",
    "            env=env, red_policy=pretrain_policy, blue_policy=my_model_pretrain_policy, n_episode=30\n",
    "        )\n",
    "    )\n",
    "    print(\"=\" * 20)\n",
    "\n",
    "    print(\"Eval with final trained policy\")\n",
    "    print(\n",
    "        run_eval(\n",
    "            env=env, red_policy=final_pretrain_policy, blue_policy=my_model_pretrain_policy, n_episode=30\n",
    "        )\n",
    "    )\n",
    "    print(\"=\" * 20)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DB_wxd59NdZo",
    "trusted": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "isSourceIdPinned": true,
     "modelId": 193174,
     "modelInstanceId": 170865,
     "sourceId": 200290,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30805,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "python3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
